{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23 February 2021\n",
    "\n",
    "Last updated 26 February 2021\n",
    "\n",
    "An `ipywidgets`-based Algonquian-English/English-Algonquian translation gui.\n",
    "\n",
    "To run in a browser window, use the following command in the command line:\n",
    "\n",
    "> `voila gui.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import ipywidgets as widgets\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = pandas.read_csv('entries.csv',header=1)\n",
    "\n",
    "# eng2alg_dict good cause engs are unique\n",
    "eng2alg_dict = entries.set_index('English').to_dict()['Algonquian']\n",
    "\n",
    "# alg2eng_dict BAD cause algs are not unique\n",
    "alg2eng_dict = {}\n",
    "for eng, alg in eng2alg_dict.items():\n",
    "    if alg not in alg2eng_dict.keys():\n",
    "        alg2eng_dict[alg] = [eng]\n",
    "    else:\n",
    "        alg2eng_dict[alg].append(eng)\n",
    "\n",
    "# eng2analysis_dict GOOD cause engs are unique\n",
    "eng2analysis_dict = entries.set_index('English').to_dict()['Analysis']\n",
    "\n",
    "# alg2analysis_dict BAD cause algs are not unique \n",
    "alg2analysis_dict = {}\n",
    "for alg, engs in alg2eng_dict.items():\n",
    "    alg2analysis_dict[alg] = [eng2analysis_dict[eng] for eng in engs]\n",
    "    \n",
    "gloss2eng_dict = entries.set_index('Gloss').to_dict()['English']\n",
    "gloss2alg_dict = entries.set_index('Gloss').to_dict()['Algonquian']\n",
    "\n",
    "analysis2eng_tup = list(zip(entries['Analysis'],entries['English']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossstring2alg_tup = []\n",
    "for gloss, alg in gloss2alg_dict.items():\n",
    "    glossstring = re.sub('^([a-z]|_)+(?!12ID)','',gloss)\n",
    "    glossstring2alg_tup.append((glossstring,alg))\n",
    "    \n",
    "all_glossstrings = [gs[0] for gs in glossstring2alg_tup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "glossary = pandas.read_csv('glossary.csv')\n",
    "\n",
    "stems_only_alg2eng = dict(zip(glossary['TA'].tolist(), glossary['eng_updated_210129']))\n",
    "stems_only_alg2eng.update(dict(zip(glossary['TI'].tolist(), glossary['eng_updated_210129'])))\n",
    "stems_only_alg2eng.update(dict(zip(glossary['AI'].tolist(), glossary['eng_updated_210129'])))\n",
    "stems_only_alg2eng.update(dict(zip(glossary['II'].tolist(), glossary['eng_updated_210129'])))\n",
    "del stems_only_alg2eng['-']\n",
    "\n",
    "alg_stems = list(set(glossary['TA'].tolist()+glossary['TI'].tolist()+glossary['AI'].tolist()+glossary['II'].tolist()))\n",
    "alg_stems.remove('-')\n",
    "alg_stems = [i.split('/') for i in alg_stems]\n",
    "alg_stems = [i for j in alg_stems for i in j]\n",
    "\n",
    "eng_stems_original = glossary['eng_updated_210129'].tolist()\n",
    "eng_stems_fixed = [re.sub('  ',' ', ' '.join(e.split('_'))) for e in eng_stems_original]\n",
    "\n",
    "eng_transitivity_TA = dict(zip(glossary['TA'].tolist(),glossary['eng_updated_210129']))\n",
    "eng_transitivity_TA = {v:'TA' for k,v in eng_transitivity_TA.items() if k != '-'}\n",
    "\n",
    "eng_transitivity_TI = dict(zip(glossary['TI'].tolist(),glossary['eng_updated_210129']))\n",
    "eng_transitivity_TI = {v:'TI' for k,v in eng_transitivity_TI.items() if k != '-'}\n",
    "\n",
    "eng_transitivity = dict(eng_transitivity_TA, **eng_transitivity_TI)\n",
    "\n",
    "needs_someone = [e for e in eng_stems_original if '__' in e and eng_transitivity[e]=='TA']\n",
    "needs_something = [e for e in eng_stems_original if '__' in e and eng_transitivity[e]=='TI']\n",
    "\n",
    "insert_someone = {(re.sub('__',' someone ',e)):e for e in eng_stems_original if e in needs_someone}\n",
    "insert_someone = {re.sub('_',' ',k):v for k,v in insert_someone.items()}\n",
    "insert_something = {re.sub('__',' something ',e):e for e in eng_stems_original if e in needs_something}\n",
    "insert_something = {re.sub('_',' ',k):v for k,v in insert_something.items()}\n",
    "\n",
    "eng_stems_map = dict(zip(eng_stems_fixed, eng_stems_original))\n",
    "eng_stems_map.update(insert_someone)\n",
    "eng_stems_map.update(insert_something)\n",
    "\n",
    "capitalization_map = dict(zip([e.lower() for e in eng2alg_dict.keys()],eng2alg_dict.keys()))\n",
    "\n",
    "punctuation = \"\\\"'.,!?\"\n",
    "\n",
    "eng_words = []\n",
    "for sentence in eng2alg_dict.keys():\n",
    "    new_words = sentence.split()\n",
    "    eng_words.extend([w.lower() for w in new_words if w.lower() not in eng_words])\n",
    "    \n",
    "valid_words = eng_words + list(eng2alg_dict.values()) + ['will']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_in_tuple(key,tuple_set):\n",
    "    for tup in tuple_set:\n",
    "        if tup[0] == key:\n",
    "            return tup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_input(text):\n",
    "    text = text.strip().lower()\n",
    "    text = ''.join([c for c in text if c not in punctuation])\n",
    "    text = re.sub('dont',\"do not\", text)\n",
    "    text = re.sub('you guys','yall',text)\n",
    "    text = re.sub(' the ',' that ',text)\n",
    "    text = re.sub(r'\\s\\s+',' ',text)\n",
    "    \n",
    "    if text in capitalization_map.keys():\n",
    "        text = capitalization_map[text]\n",
    "    \n",
    "    input_type = ('none','none')\n",
    "        \n",
    "    if text in alg2eng_dict.keys():\n",
    "        input_type = ('conjugated', 'algonquian')\n",
    "        \n",
    "    elif text in eng2alg_dict.keys():        \n",
    "        input_type = ('conjugated','english')\n",
    "        \n",
    "    elif text in alg_stems:\n",
    "        input_type = ('stem', 'algonquian')\n",
    "        \n",
    "    elif text in eng_stems_map.keys():\n",
    "        input_type = ('stem', 'english')\n",
    "        \n",
    "    return text, input_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e77db2ca9eba4b8386afc0866c71da4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1><b>Algonquian Dictionary</b></h>'), HTML(value='<h2><b>Translate</b></h>'), Tex…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def translate(text):\n",
    "    language = None\n",
    "    warning = None\n",
    "    \n",
    "    text = clean_input(text)[0]\n",
    "    \n",
    "    text = re.sub(\"y'all\",\"yall\",text)\n",
    "\n",
    "    if clean_input(text)[1][0] not in ['conjugated','none']:\n",
    "        return \"This looks like a single verb stem.\\nTry using the Search function instead.\"\n",
    "         \n",
    "    if text in capitalization_map.keys():\n",
    "        text = capitalization_map[text]\n",
    "    \n",
    "    if text in alg2eng_dict.keys():\n",
    "        language = 'Algonquian'\n",
    "        translations = alg2eng_dict[text]\n",
    "        analyses = alg2analysis_dict[text]\n",
    "        \n",
    "    elif text in eng2alg_dict.keys():        \n",
    "        language = 'English'\n",
    "        \n",
    "        r_we = re.compile('^we(?!-)|\\swe(?!-)')\n",
    "        r_youyall = re.compile('.*you.*|.*yall.*')\n",
    "        if r_we.match(text.lower()) and not r_youyall.match(text.lower()):\n",
    "            warning = 'Heads up! It looks like you\\'ve used the word \"we.\" If you mean \"we'+ \\\n",
    "                        ' (including the person I\\'m talking to)\", then try using \"we-inc\" instead.'\n",
    "\n",
    "        r_us = re.compile('.*us(?!-)$|.*\\sus(?!-)\\s')\n",
    "        if r_us.match(text.lower()) and not r_youyall.match(text.lower()):\n",
    "            warning = 'Heads up! It looks like you\\'ve used the word \"us.\" If you mean \"us'+ \\\n",
    "                        ' (including the person I\\'m talking to)\", then try using \"us-inc\" instead.'\n",
    "        \n",
    "        translations = [eng2alg_dict[text]]\n",
    "        analyses = [eng2analysis_dict[text]]\n",
    "        \n",
    "    else:   \n",
    "        if 'will' in text:\n",
    "                    return 'You may be trying to translate a future-tense sentence. Currently, the dictionary only ' + \\\n",
    "                           'supports present tense.\\nBut you can easily get the future tense by putting the word \"mus\"' + \\\n",
    "                           'in front of the present tense! For example:\\n' + \\\n",
    "                           '\\t\"I see you.\"\\t\\t==> \"kunáwush\"\\n' + \\\n",
    "                           '\\t\"I will see you.\"\\t==> \"mus kunáwush\"'\n",
    "    \n",
    "        for word in text.split(' '):\n",
    "            if word not in valid_words or word in ['something']:\n",
    "                \n",
    "                not_allowed = ['it','thing','something']\n",
    "                for w in not_allowed:\n",
    "                    if ' '+w in text or re.compile('^'+w).match(text):\n",
    "                        return 'It looks like you used the word \"'+w+'.\" Try replacing this word with one of the following:\\n' +\\\n",
    "                                '\\n\\t'.join(['\\tthat NA','those NAs','that NI','those NIs',\n",
    "                                           'a NA','some NAs','a NI','some NIs'])\n",
    "                \n",
    "                pronouns = ['she','they','him','her','them','he']\n",
    "                for p in pronouns:\n",
    "                    if word == p:\n",
    "                        return 'It looks like you used the word \"'+p+'.\" Try replacing this word with one of the following:\\n' +\\\n",
    "                                '\\n\\t'.join(['\\tthat NA','those NAs','a NA','some NAs'])                \n",
    "                \n",
    "                message = \"The word \\\"\"+word+'\" is not recongized.\\n'\n",
    "                if 'a' in word or 'o' in word:\n",
    "                    message += \"Try double checking the spelling of special characters (á, ô)!\"\n",
    "                \n",
    "                return message\n",
    "            \n",
    "        return \"This phrase was not found in the dictionary.\"\n",
    "        \n",
    "    def message(translation,analysis):\n",
    "        return \"Translation:\\t\"+translation+'.\\n' + \"Break it down:\\t\"+analysis\n",
    "    \n",
    "    output = '\\n\\n'.join([message(t,a) for t,a in zip(translations,analyses)])\n",
    "    \n",
    "    num_translations = str(len(translations))\n",
    "    if num_translations == '1':\n",
    "            middle = 'is 1 possible translation'\n",
    "    else:\n",
    "            middle = 'are '+num_translations+' possible translations'\n",
    "    num_translations = 'There '+middle+' for \"'+text+'\":\\n\\n'\n",
    "    \n",
    "    output = num_translations + output\n",
    "    \n",
    "    if language:\n",
    "        output = language + ':\\t' + text + '.\\n\\n' + output\n",
    "    if warning:\n",
    "        output = warning + '\\n\\n' + output\n",
    "    \n",
    "    return output\n",
    "        \n",
    "def on_button_clicked(b):\n",
    "    response = translate(w_text.value)\n",
    "    w_output.clear_output()\n",
    "    with w_output:\n",
    "        print(response)\n",
    "        \n",
    "def get_all_forms(text):\n",
    "    \n",
    "    text = clean_input(text)[0]\n",
    "\n",
    "    if clean_input(text)[1][0] not in ['stem','none']:\n",
    "        return \"This looks like a conjugated form. Try searching for just the verb stem.\\nOr, try \" + \\\n",
    "               \"using the Translate function instead.\"\n",
    "\n",
    "    text = re.sub('^to ','',text)\n",
    "    text = re.sub('^(is|am|are) ','be ',text)\n",
    "    if 'be '+text in eng_stems_map.keys():\n",
    "        text = 'be '+text\n",
    "    if text == 'has':\n",
    "        text = 'have'\n",
    "        \n",
    "    if text not in eng_stems_map.keys() and text not in alg_stems:\n",
    "        return '\"' + text + '\" was not found in the dictionary.'\n",
    "    \n",
    "    if text in eng_stems_map.keys():\n",
    "        eng_stem_wunderscore = eng_stems_map[text]\n",
    "        rg = re.compile(r\"^\"+eng_stem_wunderscore+r\"\\..*\")\n",
    "        all_forms = [gloss2eng_dict[e] for e in gloss2eng_dict.keys() if rg.match(e)]\n",
    "        translations = [eng2alg_dict[eng] for eng in all_forms]\n",
    "        num_forms = len(all_forms)\n",
    "        \n",
    "    elif text in alg_stems:\n",
    "        # get all the analyses that start with text\n",
    "        r_pref = re.compile(r\"^(ku|nu|wu)\\+\"+text+r\"(\\+.*$|$)\")\n",
    "        r_nopref = re.compile(text+r\"(\\+.*$|$)\")\n",
    "\n",
    "        # translations should contain all the english translations of these uses\n",
    "        translations = [tup[1] for tup in analysis2eng_tup if r_pref.match(tup[0]) or r_nopref.match(tup[0])]\n",
    "\n",
    "        # all_forms should contain all the algonquian uses of mis\n",
    "        all_forms = [eng2alg_dict[eng] for eng in translations]\n",
    "        num_forms = len(all_forms)\n",
    "        \n",
    "    all_forms = [e+'\\t\\t'+a for e,a in zip(all_forms,translations)]\n",
    "    output = '\\n'.join(all_forms)\n",
    "    message = str(num_forms)+' forms of \"'+text+'\" were found:\\n\\n'+output\n",
    "    \n",
    "    return message\n",
    "        \n",
    "def on_button_clicked2(b):\n",
    "    response = get_all_forms(w_text2.value)\n",
    "    w_output2.clear_output()\n",
    "    with w_output2:\n",
    "        print(response)\n",
    "        \n",
    "        \n",
    "def find_same_pattern(text):\n",
    "    \n",
    "    text = clean_input(text)[0]\n",
    "    \n",
    "    if clean_input(text)[1][1] == 'algonquian':\n",
    "        glosses = [tup[0] for tup in glossstring2alg_tup if tup[1]==text]\n",
    "        all_forms = []\n",
    "        translations = []\n",
    "        for gloss in glosses:\n",
    "            new_forms = [tup[1] for tup in glossstring2alg_tup if tup[0]==gloss and tup[1]!=text]\n",
    "            all_forms.append('\\n'.join(new_forms))\n",
    "            \n",
    "    output = '\\n\\n'.join(all_forms)\n",
    "    return output\n",
    "        \n",
    "def on_button_clicked3(b):\n",
    "    response = find_same_pattern(w_text3.value)\n",
    "    w_output3.clear_output()\n",
    "    with w_output3:\n",
    "        print(response)\n",
    "\n",
    "w_mainheader = widgets.HTML('<h1><b>Algonquian Dictionary</b></h>')\n",
    "        \n",
    "w_header = widgets.HTML('<h2><b>Translate</b></h>')\n",
    "w_text = widgets.Textarea(placeholder='Write something (English or Algonquian)!', layout=widgets.Layout(width ='80%'))\n",
    "w_button = widgets.Button(description='Translate')\n",
    "w_button.on_click(on_button_clicked)\n",
    "w_output = widgets.Output()\n",
    "\n",
    "w_header2 = widgets.HTML('<h2><b>See all forms of a verb</b></h>')\n",
    "w_text2 = widgets.Textarea(placeholder='Search for an English verb or an Algonquian verb stem',\n",
    "                              layout=widgets.Layout(width='30%'))\n",
    "w_button2 = widgets.Button(description='Search')\n",
    "w_button2.on_click(on_button_clicked2)\n",
    "w_output2 = widgets.Output()\n",
    "\n",
    "w_header3 = widgets.HTML('<h2><b>Find words with the same pattern</b></h>')\n",
    "w_text3 = widgets.Textarea(placeholder='Search for a conjugated Algonquian verb or English sentence', layout=widgets.Layout(width='80%'))\n",
    "w_button3 = widgets.Button(description='Search')\n",
    "w_button3.on_click(on_button_clicked3)\n",
    "w_output3 = widgets.Output()\n",
    "\n",
    "ui_items = [w_mainheader,w_header,w_text,w_button, w_output,\n",
    "            w_header2,w_text2,w_button2,w_output2,\n",
    "            w_header3,w_text3,w_button3,w_output3]\n",
    "w_ui = widgets.VBox(ui_items, layout=widgets.Layout(align_items='center'))\n",
    "display(w_ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
